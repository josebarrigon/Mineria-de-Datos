{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03ba49f7bd6e6f8ffcfc313a1103f32a",
     "grade": false,
     "grade_id": "cell-7d6c2df1488dd966",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Documentos de texto y aprendizaje automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4898ebc3bd20e986217df4eef71d0d5",
     "grade": false,
     "grade_id": "cell-3ccce721eb710dcf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Para poder aplicar técnicas de aprendizaje automático en documentos de texto es necesario transformarlos en vectores de características numéricas. Una de las formas más habituales de llevar a cabo esta tarea es mediante la representación “Bolsa de Palabras”, Bag of Words (BoW) en inglés.\n",
    "\n",
    "Para realizar el aprendizaje del modelo BoW del texto es necesario disponer de un conjunto de documentos, conocido como corpus. Cada documento está compuesto por un conjunto de palabras. \n",
    "\n",
    "Para realizar la representación BoW se debe realizar lo siguiente:\n",
    "* 1. Asignar un identificador entero (id) a cada palabra de cualquier documento del corpus. Es decir, al final habrá tantos id’s como palabras diferentes en el corpus. Esto se puede llevar a cabo construyendo un diccionario de palabras a índices enteros.\n",
    "* 2. En cada documento #i, contar el número de ocurrencias de cada palabra, p, y almacenarlo la fila i-ésima de una matriz X\n",
    "    * X[i, j] representa la característica #j, donde j es el índice (id anterior) de la palabra p en el diccionario.\n",
    "\n",
    "Por tanto, utilizar la representación BoW implica que el número de características generado (n_features) es igual al número de palabras diferentes en el corpus. Normalmente el número de características es muy grande (>100.000). \n",
    "\n",
    "En resumen, BoW implica que la matriz X tiene tantas filas como documentos en el corpus y tantas columnas como número de características. Es decir, genera tantos vectores de características (ejemplos) como documentos. Si por ejemplo el número de documentos es 10.000 y el número de características generado es 100.000, almacenar X como un array de NumPy de tipo float32 (ocupa 4 bytes) requeriría de 4GB de RAM (10.000x100.000x4). Esta cantidad de memoria empieza a ser poco manejable para los ordenares convencionales de hoy en día.\n",
    "\n",
    "Afortunadamente, muchos de los valores almacenados en X serán ceros (típicamente más del 99%) puesto que en cada documento se utilizará un subconjunto pequeño de palabras únicas que en todo el corpus. Por este motivo se dice que BoW es típicamente un conjunto de datos disperso de alta dimensionalidad (high-dimensional sparse dataset). Como consecuencia se puede ahorrar el uso de mucha memoria si solamente se almacenan las partes diferentes de cero de los vectores de características. Scikit-learn almacena la representación de BoW en matrices dispersas (scipy.sparse matrices).\n",
    "\n",
    "Scikit-learn ofrece utilidades para abordar el problema de la extracción de características numéricas a partir de documentos de texto mediante las formas más habituales: \n",
    "* Partición de una cadena de texto y asignación de un id a cada palabra. Este proceso se suelen realizar utilizando los caracteres en blanco o los signos de puntuación como separadores de palabras. Este proceso se llama en inglés **tokenizing**, puesto que cada palabra se denomina token.\n",
    "* **Conteo** del número de ocurrencias de cada palabra (token) en cada documento.\n",
    "* **Normalización y ponderación** con importancia decreciente de las palabras que aparezcan en la mayoría de los documentos.\n",
    "\n",
    "**Al proceso general de transformar una colección de documentos de texto en vectores de características numéricas se le llama vectorización. Es decir, a la estrategia compuesta por la tokenización, conteo y normalización.** Como consecuencia, los documentos están descritos por las ocurrencias de las palabras y la información relativa a las posiciones de las palabras dentro de los documentos se ignora. A continuación se muestra como se realiza cada proceso en scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53fc39a064c3e57987eb3548f440ebd8",
     "grade": false,
     "grade_id": "cell-dc4de23218b02e12",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Tokenizing y conteo con scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3f2717a8ef8e6474d7c0080e1e2fe32",
     "grade": false,
     "grade_id": "cell-3edab64282ef396b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Scikit-learn ofrece una clase que implementa las etapas de **tokenizado del texto y conteo de palabras**. Esta clase se llama [*CountVectorizer*](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) y está dentro del paquete de extracción de características para texto de scikit-learn.\n",
    "\t\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "Para entender cómo funciona esta clase vamos a crear un ejemplo de un corpus formado por 4 documentos de texto en inglés. Todos los documentos sn muy cortos para visualizar mejor el proceso y en inglés ya que es el idioma por defecto en Scikit-learn y todas las funcionalidades están más desarrolladas para dicho idioma.\n",
    "\n",
    "corpus = ['This is the first document.',\n",
    "          'This is the second second document.',\n",
    "          'And the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "Para aplicar las funcionalidades ofrecidas por CountVectorizer lo primero que hay que hacer es llamar al constructor:\n",
    "\t\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "Dicha llamada al constructor utiliza los valores por defecto de la clase. Algunos de los híper-parámetros más relevantes son:\n",
    "* stop_words: determina las palabras a ignorar en el proceso de tokenizado. Se puede establecer a ‘english’ que elimina las palabras más frecuentes del inglés o a una lista de palabras establecidas por el usuario. \n",
    "* max_df: número en el rango [0,1] que establece un umbral por el que si la frecuencia de una palabra es mayor que este valor se ignora en la construcción del diccionario. \n",
    "* min_df: número en el rango [0,1] que establece un umbral por el que si la frecuencia de una palabra es menor que este valor se ignora en la construcción del diccionario. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3813fde4cf18cc33c99f7eb2ddcf940",
     "grade": false,
     "grade_id": "cell-a4c529d180370caa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Se importa la librería para realizar la autocorrección\n",
    "from test_helper import Test\n",
    "# Se importa la librería CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Se genera el corpus, cada documento es un string y el corpus es una lista de strings (documentos)\n",
    "corpus = ['This is the first document.',\n",
    "          'This is the second second document.',\n",
    "          'And the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "# Se llama al constructor de la clase CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7701774cd74f42f662f63dcebddb1b1",
     "grade": false,
     "grade_id": "cell-c8e28595d6c59346",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Una vez generado el objeto de CountVectorizer, el siguiente paso es aprender el vocabulario (diccionario) con las palabras de nuestro corpus (se realiza el proceso de tokenizing). Para ello se llama a la función fit con el corpus de documentos:\n",
    "\n",
    "\tvectorizer.fit(corpus)\n",
    "\n",
    "Una vez aprendido el diccionario se realiza el proceso de conteo y creación de los vectores de características. Para ello se llama a la función transform pasando como argumento de entrada el corpus de documentos para realizar el conteo sobre dicho corpus:\n",
    "\n",
    "\tX = vectorizer.transform(corpus)\n",
    "\n",
    "También existe la opción de realizar las dos operaciones anteriores de una sola vez utilizando la función fit_transform:\n",
    "\n",
    "\tX = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "Los nombres de las características extraídas (palabras) se pueden obtener utilizando la función get_feature_names. Se pueden visualizar por pantalla:\n",
    "\n",
    "\tprint vectorizer.get_feature_names()\n",
    "    \n",
    "Mediante la función anterior se muestran las palabras en el orden en el que han sido aprendidas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c99636545f119dda730d5629deb2cbf6",
     "grade": false,
     "grade_id": "cell-9b20fe4eda6e9464",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Realiza el aprendizaje del vocabulario a parir del corpus creado anteriormente, transfórmalo y muestra las palabras aprendidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddcdcb24584c02d09661bce425792c67",
     "grade": false,
     "grade_id": "cell-90fab646473dd3be",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# Se realiza el aprendizaje a partir del corpus y se transforma que los valores aprendidos\n",
    "corpusTransformado = vectorizer.fit_transform(corpus)\n",
    "# Se muestran los términos aprendidos (variables del modelo BoW)\n",
    "palabrasAprendidas = vectorizer.get_feature_names()\n",
    "print(palabrasAprendidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43e9616a96ed0b1f2e6ab5e0cb9deb7e",
     "grade": true,
     "grade_id": "cell-bb28a387ed478a49",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(palabrasAprendidas, [u'and', u'document', u'first', u'is', u'one', u'second', u'the', u'third', u'this'],'Palabras aprendidas incorrectas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fde8c7e326b0318968736e1fe3bd704",
     "grade": false,
     "grade_id": "cell-27c35ffe97902fca",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Para comprobar el índice de una palabra determinada, pal, en esta lista se puede utilizar la función get sobre el atributo vocabulary_:\n",
    "\n",
    "\tprint vectorizer.vocabulary_.get(pal)\n",
    "    \n",
    "NOTA: si una palabra no está en el vocabulario aprendido el resultado es None.\n",
    "    \n",
    "Para ver las ocurrencias de cada característica en cada documento\n",
    "\n",
    "\tprint corpusTransformado.toarray()\n",
    "    \n",
    "Muestra el índice de las palabras first, and y text. Muestra las ocurrencias de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77d8201c3dbdbf3d71cb56aec9965d15",
     "grade": false,
     "grade_id": "cell-f3197261e1bce851",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "None\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Se muestran los índices de las diferentes palabras dentro del modelo aprendido\n",
    "print(vectorizer.vocabulary_.get('first'))\n",
    "print(vectorizer.vocabulary_.get('and'))\n",
    "print(vectorizer.vocabulary_.get('text'))\n",
    "# Se muestra el modelo en forma de matriz\n",
    "print(corpusTransformado.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab0fd1ff71e523bd35dd853c2cd3e6f9",
     "grade": false,
     "grade_id": "cell-60bb1739fc1c74f4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "El resultado de la transformación del corpus es una matriz de tantas filas como documentos y tantas columnas como palabras diferentes haya en todos los documentos. En este caso son 4 filas y 9 columnas.\n",
    "Si se muestra directamente el corpus transformado (print corpusTransformado) se obtiene una lista de filas con el siguiente formato:\n",
    "* (idDocumento, idPalabra) numOcurrencias, donde\n",
    "    * idDocumento es el índice del documento\n",
    "    * idPalabra es el índice de la palabra (en la lista que podéis consultar en vectorizer.get_feature_names())\n",
    "    * numOcurrencias es el número de ocurrencias de la palabra idPalabra en el documento idDocumento\n",
    "\n",
    "Ejemplo: (0,0) 3, significa que en el primer documento, la primera palabra aprendida (mostrada en vectorizer.get_feature_names()) aparece 3 veces.\n",
    "\n",
    "Muestra por pantalla el corpus transformado sin pasarlo a matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85d65b6cf4daa49431fee13ab513c36f",
     "grade": false,
     "grade_id": "cell-4c07e5a3efd09ad4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t2\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "# Se muestra el modelo aprendido sin forma matricial\n",
    "print(corpusTransformado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdb57105ddb31de81b80ea9e0ae816a6",
     "grade": false,
     "grade_id": "cell-104f79bd9b5fa53d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Para transformar un nuevo documento a esta representación, utilizando el vocabulario aprendido, se utiliza la función transform (explicada anteriormente) pasando como argumento de entrada el nuevo documento a transformar. Por ejemplo para transformar 'Something completely new.' Se ejecutaría la siguiente instrucción:\n",
    "\n",
    "    nuevoDocT = vectorizer.transform(['Something completely new.'])\n",
    "\n",
    "Para visualizar el resultado:\n",
    "\n",
    "\tprint nuevoDocT.toarray()\n",
    "\n",
    "El resultado será una lista con 9 ceros puesto que ninguna de las palabras de la frase ha sido aprendida mediante el corpus introducido. Hay 9 ceros puesto que una vez aprendido el vocabulario no se incluyen nuevas palabras a menos que se entrene de nuevo con nuevas palabras (un nuevo corpus).\n",
    "\n",
    "Realiza las dos instrucciones anteriores.  El resultado debe ser una lista con todo ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8392e9be869211b3f70451b59db31b8a",
     "grade": false,
     "grade_id": "cell-0e4d96ae4b7dc9c2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Se realiza la transformación de un nuevo documento (string) en base al corpus aprendido\n",
    "nuevoDocT = vectorizer.transform(['Something completely new.'])\n",
    "print(nuevoDocT.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fbd1402349b28e559310c2e45eb6db1",
     "grade": false,
     "grade_id": "cell-18c630bd81878633",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Crea un nuevo objeto de la clase CountVectorizer pero esta vez establece como stop-words la lista con las siguientes palabras: this, is, the, and.\n",
    "\n",
    "Aprende el vocabulario a partir del corpus inicial y muestra tanto la lista de palabras aprendidas como el corpus transformado por el nuevo vocabulario. Observa las diferencias de la transformación al utilizar esta lista de stop-words o sin usarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adbbbb9a4632fb1cad257d5bcc636e18",
     "grade": false,
     "grade_id": "cell-94302f533cb822b1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'one', 'second', 'third']\n",
      "[[1 1 0 0 0]\n",
      " [1 0 0 2 0]\n",
      " [0 0 1 0 1]\n",
      " [1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Se genera otro modelo utilizando una lista de palabras como stop-words (constructor, aprendizaje y transformación)\n",
    "    # No se tienen en cuenta como posibles variables del modelo\n",
    "vectorizer1 = CountVectorizer(stop_words=['this','is','the','and'])\n",
    "corpusTransformado1 = vectorizer1.fit_transform(corpus)\n",
    "\n",
    "# Se muestran las variables del nuevo modelo y el modelo en forma matricial\n",
    "palabrasAprendidas1 = vectorizer1.get_feature_names()\n",
    "print(palabrasAprendidas1)\n",
    "print(corpusTransformado1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1a9114da66c01639d3d59b82cee417f",
     "grade": true,
     "grade_id": "cell-0ef36de0e18de2ec",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(palabrasAprendidas1, [u'document', u'first', u'one', u'second', u'third'],'Palabras aprendidas incorrectas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1eeaf99bd579d2febde9e1edaf688e8",
     "grade": false,
     "grade_id": "cell-b18fded416fbf9b1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Crea un nuevo objeto de la clase CountVectorizer pero esta vez establece como stop-words las del idioma inglés. Vuelve a mostrar tanto la lista de palabras aprendidas como el corpus transformado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c7a3296bf8bb4bac3e5338f872070a9",
     "grade": false,
     "grade_id": "cell-3a8a878e0b00d213",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'second']\n",
      "[[1 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Se genera otro modelo utilizando la lista de stop-words del inglés (constructor, aprendizaje y transformación)\n",
    "    # No se tienen en cuenta como posibles variables del modelo\n",
    "vectorizer2 = CountVectorizer(stop_words='english')\n",
    "corpusTransformado2 = vectorizer2.fit_transform(corpus)\n",
    "\n",
    "# Se muestran las variables del nuevo modelo y el modelo en forma matricial\n",
    "palabrasAprendidas2 = vectorizer2.get_feature_names()\n",
    "print(palabrasAprendidas2)\n",
    "print(corpusTransformado2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34ae3b43c339afcfb29995b602a421f9",
     "grade": true,
     "grade_id": "cell-7c3a35584d32b553",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(palabrasAprendidas2, [u'document', u'second'],'Palabras aprendidas incorrectas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6e5b9320f4b6818b790e2a9902a533d",
     "grade": false,
     "grade_id": "cell-83809a81b557359c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### De ocurrencias a frecuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b852eca09a1a0f0a427e7bab5f92c72f",
     "grade": false,
     "grade_id": "cell-d1fc8ff4a6b89aa5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "El conteo de ocurrencias de las palabras es un buen punto de inicio pero hay un problema: los documentos largos tendrán conteos más grandes que los cortos (incluso si tratan el mismo tema).\n",
    "Para evitar este problema basta con dividir el número de ocurrencias de cada palabra en un documento por el número total de palabras del documento. A estas nuevas características se les llama frecuencias (Term Frequencies, TF, en inglés).\n",
    "Posteriormente, las frecuencias se suelen refinar de tal modo que la importancia de las palabras que aparezcan en muchos documentos del corpus sea disminuida. El motivo es que las palabras que aparecen en muchos documentos del corpus (como preposiciones por ejemplo) son menos informativas que las que aparecen en una porción pequeña de ellos. A este proceso de refinado se le llama TF-IDF: Term Frequency times Inverse Document Frequency. Es decir, se multiplica la frecuencia, TF, por la inversa de la frecuencia en todos los documentos (IDF por sus siglas en inglés):\n",
    "\n",
    "   $tf-idf(t,d) = tf(t)*idf(t)$\n",
    " \n",
    "donde t es la palabra a normalizar y d es el documento en el que se realiza la normalización. La parte correspondiente a la inversa de la frecuencia del documento se calcula como\n",
    "\n",
    "   $idf(t) = log(\\frac{1+n_d}{1+df(d,t)})$\n",
    " \n",
    "donde $n_d$ es el número total de documentos en el corpus y $df(d,t)$ es el número de documentos que contienen la palabra $t$. \n",
    "Finalmente, una vez realizado el cálculo de TF-IDF para todas las palabras de un documento, los valores resultantes de cada documento se normalizan por la norma Euclídea\n",
    "\n",
    "  $w_{norm}=\\frac{v}{||v||_2}=\\frac{v}{\\sqrt(v_1^2+...+v_n^2)}$\n",
    "\n",
    "Scikit-learn ofrece una clase que realiza todos estos cálculos a partir de una matriz de conteos (la devuelta por la clase CountVectorizer). Esta clase se llama [*TfidfTransformer*](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) y está dentro del paquete de extracción de características para documentos de texto.\n",
    "\n",
    "\tfrom sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "Para poder utilizar todas las funciones de dicha clase lo primero que se debe hacer es crear un objeto de dicha clase. Los valores por defecto del constructor realizan el proceso explicado anteriormente:\n",
    "\n",
    "\tnormalizar = TfidfTransformer()\n",
    "\n",
    "Una vez creado el objeto de la clase lo primero que se debe realizar es aprender de los datos del conteo para poder realizar las normalizaciones. Para ello se utiliza la función fit utilizando como variable de entrada una matriz de conteos (obtenida con CountVectorizer)\n",
    "\t\n",
    "\tnormalizar.fit(matrizConteo)\n",
    "\n",
    "Una vez realizado el aprendizaje se puede utilizar el objeto para llevar a cabo la obtención de los pesos correspondientes a cada palabra a partir de la matriz de conteos. Para ello se utiliza la función transform\n",
    "\n",
    "\tBoW = normalizar.transform(matrizConteo)\n",
    "\n",
    "Las dos funciones anteriores se pueden realizar de una vez utilizando la función fit_transform\n",
    "\n",
    "\tBoW = normalizar.fit_transform(matrizConteo)\n",
    "    \n",
    "Para mostrar el modelo BoW aprendido tras realizar todas las operaciones podemos utilizar la función toarray() del objeto tal y como hemos realizado con el objeto de la clase CountVectorizer.\n",
    "\n",
    "Realiza las instrucciones anteriores y muestra el modelo BoW obtenido para la configuración por defecto de la clase CountVectorizer (variable corpusTransformado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a27ddcd24d1602115ce0fcedfaed81e",
     "grade": false,
     "grade_id": "cell-3475fcf28026e340",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "# Se importa la librería en la que está la clase TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Se llama al constructor de la clase TfidfTransformer\n",
    "tdidf = TfidfTransformer()\n",
    "# Se realiza el aprendizaje de los pesos y la transformación del corpus en base a la matriz de conteos obtenida anteriormente\n",
    "    # Utilizar la matriz de conteos obtenida por CountVectorizer con los parámetros por defecto\n",
    "BoW = tdidf.fit_transform(corpusTransformado)\n",
    "# Se muestra el modelo BoW aprendido en forma matricial\n",
    "print(BoW.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c3b3a73cca7812175b2597b30b7142a",
     "grade": false,
     "grade_id": "cell-bbab6d8d2e938fee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Aprendizaje de un clasificador a partir de la representación BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82a6d6798be1d2348fb8fde35232beda",
     "grade": false,
     "grade_id": "cell-6dab46f5653dfeb8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "La representación BoW crea por cada documento de texto un ejemplo. Es decir, un vector de características (valores TF-IDF). Por tanto, todos los documentos de texto estarán representados por un vector con el mismo número de características. Como resultado, es posible realizar el aprendizaje de un clasificador utilizando la representación BoW de cada documento junto con la clase a la que pertenece dicho documento. Las clases de los documentos serán diferentes según el problema a abordar como la categoría del texto (deporte, economía, política, etc..), el sentimiento del texto (opinión positiva, negativa o neutra), entre otros problemas que se pueden afrontar.\n",
    "\n",
    "Para resolver este tipo de problemas las etapas a realizar son las siguientes:\n",
    "* Leer los ejemplos de entrenamiento del problema y formar el corpus.\n",
    "* Realizar el tokenizado y conteo del corpus.\n",
    "* Realizar la transformación a frecuencias del conteo obtenido.\n",
    "* Aprender el clasificador (cualquier clasificador) con los datos resultantes.\n",
    "* Leer los ejemplos de test del problema y formar el corpus de test.\n",
    "* Realizar el tokenizado y conteo de este corpus utilizando lo aprendido con el corpus de entrenamiento.\n",
    "* Realizar la transformación a frecuencias del nuevo conteo con lo aprendido con el corpus de entrenamiento.\n",
    "* Predecir la clase de los documentos utilizando el clasificador aprendido.\n",
    "* Calcular el porcentaje de acierto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6a7c5f1a730b039d27a39447736bbaf",
     "grade": false,
     "grade_id": "cell-178ff81ba99fcf3e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Práctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d11c22f981553d4cad0f2ca6d925f6f",
     "grade": false,
     "grade_id": "cell-b8e9795fd478e105",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Para realizar esta práctica vamos a utilizar un dataset que contiene documentos de texto ofrecido por scikit_learn. Este dataset se llama [Twenty Newsgroups](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups). Su descripción oficial es la siguiente: \n",
    "\n",
    "    \"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "\n",
    "Para trabajar con este dataset lo primero que debemos hacer es importarlo:\n",
    "\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "Una vez importado se leerán los datos de los documentos. Para ello, la llamada al constructor de la clase es la siguiente:, se establece como subset la opción train y de esta forma devuelve los datos de entrenamiento (poner esta opción a test para leer los datos de test).\n",
    "\n",
    "    twenty_train = fetch_20newsgroups(subset=tipoDatos, shuffle=aleatorio, random_state=semilla, categories=clasesDocumentos)\n",
    "\n",
    "Los parámetros son los siguientes:\n",
    "* tipoDatos: string que determina si los datos son de entrenamiento (asignar 'train') o de test (asignar 'test')\n",
    "* aleatorio: valor booleano que establece si se aleatorizan los datos o no.\n",
    "* semilla: valor entero que determina la semilla para la generación de números aleatorios. De esta forma los experimentos serán reproducibles.\n",
    "* clasesDocumentos: lista con los nombre de las clases de los docuemntos a leer. Si se asigna a None se leen los documentos de todas las clases.\n",
    "\n",
    "El objeto generado (variable twenty_train) tiene la misma estructura que todos los datasets nativos de Scikit-learn con los que hemos trabajado. Por tanto:\n",
    "* Los **datos de entrada** correspondientes a los documentos de texto se encuentran en el campo **data**. Por ejemplo, si se desea mostrar el primer documento se puede ejecutar:\tprint(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))\n",
    "* Los **nombres de las clases** se encuentran en el campo **target_names**. En este corpus de documentos se encuentran documentos clasificados en 20 clases diferentes, para visualizarlos ejecuta: print twenty_train.target_names\n",
    "* Las **clases** de cada documento se encuentran en el campo **target**. Por ejemplo si se quiere obtener la clase del primer documento se ejecutaría: print twenty_train.target[0]\n",
    "* Si anidamos los campos target_names y target podemos visualizar la clase de cada documento. Por ejemplo si se desea conocer la clase del primer documento se ejecutaría: print twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88dfbfc3cb6c61ef9fa8d2d5573f91f0",
     "grade": false,
     "grade_id": "cell-664a9cbec1cbb107",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ejercicio 1: lee los datos de entrenamiento (utiliza el valor 42 como semilla para la lectura) correspondientes a las clases de nombre 'alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med'. Muestra el contenido del primer documento, su clase y el nombre de dicha clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2222ec3dc827cad417f551e074b6ca70",
     "grade": false,
     "grade_id": "cell-5bff82c6f5937edf",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n",
      "1\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Se importa el dataset fetch_20newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# se realiza la lectura de los datos de entrenamiento: poner el parámetro shuffle a True y utilizar las clases apropiadas\n",
    "clases = ['alt.atheism','soc.religion.christian','comp.graphics','sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',shuffle=True,categories=clases,random_state=42)\n",
    "\n",
    "# Se muestra el primer documento (mail) junto con su clase y el nombre de la misma\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))\n",
    "print(twenty_train.target[0])\n",
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83bd73289424960b30e3244999c25ca6",
     "grade": false,
     "grade_id": "cell-2f92607a5cf2d3c0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ejercicio 2: Realiza el proceso de tokenizado, conteo y obtención de los pesos (td-idf) del corpus generado (el resultado de la lectura)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb39ab4a3bab1302235a65ef18dce543",
     "grade": false,
     "grade_id": "cell-36f9150771f5cf91",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n"
     ]
    }
   ],
   "source": [
    "# Se importa la librería para realizar la autocorrección\n",
    "from test_helper import Test\n",
    "\n",
    "# Se crea el objeto de la clase CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "# Realizar el aprendizaje y la transformación a partir de los datos de entrenamiento leídos anteriormente\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "# Se crea el objeto de la clase TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# Realizar el aprendizaje y la transformación a partir de la matriz de conteo generada anteriormente\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# Se muestran las dimensiones de la matriz que representa al modelo BoW aprendido\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "def5e7677692ce245655e4401a13b43f",
     "grade": true,
     "grade_id": "cell-f1baa331a1b819f7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(X_train_tfidf.shape, (2257,35788), 'Modelo BoW con dimensiones incorrectas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "153f3b8ce3f544b6846e2201c8bf8652",
     "grade": false,
     "grade_id": "cell-61d824604587ca6c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Una vez realizado el proceso anterior se va a realizar el aprendizaje de un clasificador. En este caso vamos a utilizar la versión multinomial de **Naïve Bayes** ya que en la literatura clásica se suele utilizar este clasificador en conjunción con el modelo BoW para afontar problemas de minería de textos. Pese a que no lo hemos visto en esta asignatura, el clasificador Naïve Bayes es un clasificador bastante sencillo que se basa en el teorema de Bayes para realizar el aprendizaje del modelo de clasificación. La librería Scikit-learn nos ofrece dicho clasificador para poder utlizarlo fácilmente ya que nos bastará con invocar su constructor, su método de entrenamiento (*fit*) y su método de predicción de la clase de nuevos ejemplos (*predict*). Para utilizar este clasificador, llamado [*MultinomialNB*](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) primero hay que importarlo\n",
    "\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "Los híper-parámetros más importantes de esta clase son:\n",
    "* alpha: valor real que determina la constante para realizar el suavizado de Laplace. Por defecto el valor es 1.0. Si no se quiere realizar suavizado asignar el valor 0.0.\n",
    "* fit_prior: valor booleano que determina si se aprenden las probabilidades a priori de las clases o no. El valor por defecto es True.\n",
    "\n",
    "Ejercicio 3: Crear un objeto de MultinomialNB con los valores por defecto. Realizar el aprendizaje (fit) con el modelo BoW obtenido en el ejercicio 2 y las clases correspondientes a los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f20ba26d19bf2394938fc3da84c6e53c",
     "grade": false,
     "grade_id": "cell-193c311fd9e0f193",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se importa la librería de la clase Naïve Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Se realiza el aprendizaje del clasificador Naïve Bayes\n",
    "clf_NB = MultinomialNB()\n",
    "#Entrenamos el modelo\n",
    "clf_NB.fit(X_train_tfidf,twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22345bd658cc9f0a5152dc2d4014dadf",
     "grade": false,
     "grade_id": "cell-cc624839f4c2b5c8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ejercicio 4: Leer los datos de test utilizando el valor 42 como semilla. Realizar el proceso de tokenizado, conteo y obtención de los pesos de cada palabra (td-idf) del corpus generado con los parámetros aprendidos en el ejercicio 2. Realizar la predicción de los ejemplos con el calificador aprendido en el ejercicio 3. Obtener su porcentaje de acierto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3e192a3ea9b3583df4081a0acf06750",
     "grade": false,
     "grade_id": "cell-06ef14260a0b9e4a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.49\n"
     ]
    }
   ],
   "source": [
    "# Se importa la librería metrics para calcular el rendimiento de los clasificadores\n",
    "from sklearn import metrics\n",
    "\n",
    "# Se leen los datos de test: asignar el parámetro shuffle a True y utilizar las clases apropiadas\n",
    "twenty_test =  fetch_20newsgroups(subset='test', shuffle=True, categories=clases,random_state=42)\n",
    "\n",
    "# Realizar la transformación de los documentos de test tanto con CountVectorizer como con TfidfTransformer\n",
    "X_test_counts = count_vect.transform(twenty_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# Realizar la predicción de los datos de test utiliando el clasificador Naïve Bayes aprendido anteriormente\n",
    "prediccionNB_test = clf_NB.predict(X_test_tfidf)\n",
    "# Se calcula e imprime el porcentaje de acierto (entre 0 y 100)\n",
    "accuracy = round(metrics.accuracy_score(twenty_test.target,prediccionNB_test)*100,2)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e20104c7dbb03e4744b31610d97dea4",
     "grade": true,
     "grade_id": "cell-d09f7a6739b1eebc",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(round(accuracy,2), 83.49, 'Accuracy en test obtenido por Naïve Bayes incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d309fa2e3ec5e5ec3100c5b08109b19",
     "grade": false,
     "grade_id": "cell-41f451633930e155",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ejercicio 5: crear una Pipeline compuesta por los 3 objetos necesarios para resolver el problema de predecir la clase de los documentos de texto (CountVectorizer, TfidfTransformer y MultinomialNB). Entrenarla con los datos de entrenamiento, predecir las clases de los datos de test y obtener su porcentaje de acierto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76f338de5ea5b3048853f3c8f1f51b82",
     "grade": false,
     "grade_id": "cell-0a6201c3c9aa7ce3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.49\n"
     ]
    }
   ],
   "source": [
    "# Se importa la librería para utilizar la clase Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Se crea la pipeline con las 3 fases necesarias para solventar el problema\n",
    "text_clf = Pipeline([('countvectorizer',CountVectorizer()),('tfidtransformer',TfidfTransformer()),('multinomialdb',MultinomialNB())])\n",
    "# Se realiza el aprendizaje de todos los objetos de la pipeline\n",
    "text_clf.fit(twenty_train.data,twenty_train.target)\n",
    "\n",
    "# Se realiza la predicción de los datos de test\n",
    "prediccionPipeline_test = text_clf.predict(twenty_test.data)\n",
    "# Se calcula e imprime el porcentaje de acierto (entre 0 y 100)\n",
    "accuracyPipeline = round(metrics.accuracy_score(twenty_test.target,prediccionPipeline_test)*100,2)\n",
    "print(accuracyPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc7e15a781e35b8a9d7f95fc3b9136c3",
     "grade": true,
     "grade_id": "cell-86f081b5cddc9cfe",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "Test.assertEquals(round(accuracyPipeline,2), 83.49, 'Accuracy en test obtenido por la Pipeline incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0ede5db2756b6b4ebacb906aaf934df",
     "grade": false,
     "grade_id": "cell-2db623bc3da5b9fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Ejercicio 6: utiliza la función *GridSearchCV* de la librería *model_selection* para buscar la configuración óptima del clasificador compuesto aplicando la validación cruzada de 10 particiones y obtened el rendimiento de la mejor configuración en train y test. Los híper-parámetros y valores a optimizar son:\n",
    "* CountVectorizer\n",
    "    * stop_words: ‘english’, None\n",
    "* TfidfTransformer\n",
    "    * use_idf: True, False\n",
    "* MultinomialNB \n",
    "    * alpha: 0.5, 1, 2\n",
    "    * fit_prior: True, Flase\n",
    "    \n",
    "Fijad la semilla de Numpy a 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6386034a16971f1557b95e249927bba",
     "grade": false,
     "grade_id": "cell-849e859a856c4d75",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9698682399213373\n",
      "99.46832077979619 91.34487350199734\n"
     ]
    }
   ],
   "source": [
    "# Se importa la librería para poder hacer la selección de los valores de los parámetros con validación cruzada\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(12)\n",
    "# Se crea la pipeline con las 3 fases necesarias para solventar el problema\n",
    "text_clf_pip = Pipeline([('countvectorizer',CountVectorizer()),('tfidtransformer',TfidfTransformer()),('multinomialdb',MultinomialNB())])\n",
    "# Se crea el grid de híper-parámetros (diccionario)\n",
    "parameters_NB = {'multinomialdb__alpha':[0.5,1,2],'multinomialdb__fit_prior':[True, False],'tfidtransformer__use_idf':[True, False],'countvectorizer__stop_words':['english', None]}\n",
    "\n",
    "np.random.seed(12)\n",
    "# Se llama al constructor de la clase GridSearchCV\n",
    "gs_clf_nb = model_selection.GridSearchCV(text_clf_pip,parameters_NB,cv=10)\n",
    "# Se realiza el aprendizaje utilizando la clase GridSearchCV con los 500 primeros documentos de train\n",
    "gs_clf_nb.fit(twenty_train.data,twenty_train.target)\n",
    "\n",
    "\n",
    "# Almacenamos el DataFrame con los resultados\n",
    "diccionarioResultados = pd.DataFrame(gs_clf_nb.cv_results_)\n",
    "\n",
    "# Se imprime el mejor porcentaje de acierto y los resultados de todas las configuraciones\n",
    "print(gs_clf_nb.best_score_)\n",
    "\n",
    "# Se obtiene el rendimiento en entrenamiento y test por la mejor configuración\n",
    "prediccionesTrain = gs_clf_nb.predict(twenty_train.data)\n",
    "prediccionesTest = gs_clf_nb.predict(twenty_test.data)\n",
    "accTrain = metrics.accuracy_score(twenty_train.target,prediccionesTrain)*100\n",
    "accTest = metrics.accuracy_score(twenty_test.target,prediccionesTest)*100\n",
    "print(accTrain,accTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0350217bd31990b8dfa8c2fd8540667d",
     "grade": true,
     "grade_id": "cell-e060c7d00cef2bb4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# ESTA CELDA DARÁ ERROR SI EL RESULTADO NO ES CORRECTO\n",
    "    # EN CASO CONTRARIO NO TENDRÁ SALIDA\n",
    "Test.assertEquals(round(accTrain, 2), 99.47, 'Valor de accuracy en train incorrecto')\n",
    "Test.assertEquals(round(accTest, 2), 91.34, 'Valor de accuracy en test incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La mejor configuración es correcta, pero el orden no es el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'countvectorizer__stop_words': 'english', 'multinomialdb__alpha': 0.5, 'multinomialdb__fit_prior': False, 'tfidtransformer__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "indice = np.argmax(diccionarioResultados['mean_test_score'])\n",
    "print(diccionarioResultados['params'][indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c63cb6168b3ede10bdda827a3736918a",
     "grade": true,
     "grade_id": "cell-39b54599772876a8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test failed. Mejor configuración incorrecta\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Mejor configuración incorrecta",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4b8aa777c876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mindice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiccionarioResultados\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertEquals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiccionarioResultados\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9699\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Accuracy de la mejor configuración incorrecto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertEquals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiccionarioResultados\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'clf__alpha'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clf__fit_prior'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tfidf__use_idf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vect__stop_words'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'english'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mejor configuración incorrecta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documentos/Mineria/Practicas/Mineria-de-Datos/Practica6/test_helper.py\u001b[0m in \u001b[0;36massertEquals\u001b[0;34m(cls, var, val, msg)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massertEquals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Mineria/Practicas/Mineria-de-Datos/Practica6/test_helper.py\u001b[0m in \u001b[0;36massertTrue\u001b[0;34m(cls, result, msg)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1 test failed. \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0;31m# if cls.failFast:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# if cls.private:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Mejor configuración incorrecta"
     ]
    }
   ],
   "source": [
    "# ESTA CELDA DARÁ ERROR SI EL RESULTADO NO ES CORRECTO\n",
    "    # EN CASO CONTRARIO NO TENDRÁ SALIDA\n",
    "indice = np.argmax(diccionarioResultados['mean_test_score'])\n",
    "Test.assertEquals(round(np.max(diccionarioResultados['mean_test_score']), 4), 0.9699, \"Accuracy de la mejor configuración incorrecto\")\n",
    "Test.assertEquals(diccionarioResultados['params'][indice], {'clf__alpha': 0.5, 'clf__fit_prior': False, 'tfidf__use_idf': True, 'vect__stop_words': 'english'}, \"Mejor configuración incorrecta\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
